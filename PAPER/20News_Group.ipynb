{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Morning and welcome to  <font color='BLUE'>\"Nugget Word embedding for text classification\" </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Zied **HAJ YAHIA** and Adrien **SIEG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.A - Introducing paper <font color='RED'>\"Unsupervised Text Classification Leveraging Experts and Word Embeddings\" </font> submitted to ACL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author** : Zied **HAJ YAHIA**, Adrien **SIEG**, and LÃ©a **DELERIS** (Head of RiskAir DataLab) and the great support of Jordan **TOH**\n",
    "\n",
    "- **Mission** : ORAIA (Operational Risk Artificial Intelligence Assistant) in BNP Paribas (Risk ORC)\n",
    "\n",
    "- **Year** : 2018-2019\n",
    "\n",
    "- **Topic**: Classify documents into hundreds of labels when you have NO DATA, no training set, ... nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"paper.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.B - Structure/Frame of Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Introduction**\n",
    "\n",
    "\n",
    "\n",
    "2. **Related Work**\n",
    "\n",
    "\n",
    "\n",
    "3. **Method**\n",
    "\n",
    "    - <font color='GREEN'>3.1</font> Cleaning Steps\n",
    "    - <font color='GREEN'>3.2</font> Enrichment\n",
    "    - <font color='GREEN'>3.3</font> Consolidation\n",
    "    - <font color='GREEN'>3.4</font> Text Similarity Metric\n",
    "    \n",
    "    \n",
    "    \n",
    "4. **Experiments**\n",
    "\n",
    "    - <font color='GREEN'>4.1</font> Dataset\n",
    "        - 20 NewsGroup\n",
    "        - AG's Corpus\n",
    "        - Yahoo Answer's\n",
    "        - 5 Abstract Group\n",
    "        - Google Snippets\n",
    "            \n",
    "    - <font color='GREEN'>4.2</font> Configurations and Baseline Methods\n",
    "    - <font color='GREEN'>4.3</font> Experimental Settings\n",
    "    - <font color='GREEN'>4.4</font> Results and Discussion\n",
    "    \n",
    "    \n",
    "    \n",
    "5. **Application to Operational Risk Incident Classification**\n",
    "\n",
    "  - <font color='GREEN'>5.1</font> Operational Risk Incidents Corpus and Taxonomy\n",
    " **ORAIA Dataset from BNP Group**\n",
    " \n",
    "**IFS** > Cardif, Asset Management, Wealth Management, Personal Finance, IRB, ...\n",
    "\n",
    "**Domestic Market** > Leasing Solutions, BDDF, BDDB, BNL, BGL, ...\n",
    "\n",
    "**CIB** > ITO, APAC, Global Market, BP2S / Securities Services\n",
    "\n",
    "   - <font color='GREEN'>5.2</font> Result\n",
    "   - <font color='GREEN'>5.3</font> Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.C - What is the method? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach for **unsupervised text classification** is based on the choice to model the task as a **text similarity problem** between **two sets of words**: \n",
    "\n",
    "- One containing the most relevant words in the document and \n",
    "- another containing keywords derived from the label of the target category. \n",
    "\n",
    "While the key advantage of this approach is its **simplicity**, its success hinges on the **good definition of a dictionary of words for each category.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"method.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.D - Why is it so hard? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"religions.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hockey.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.E - What is taxonomy of Operational Risk? Example with ICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_taxonomy_ict.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A concrete example with *20News Group*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "def twenty_newsgroup_to_csv():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "    \n",
    "    df['target'] = df['target'].apply(int)\n",
    "\n",
    "    targets = pd.DataFrame( newsgroups_train.target_names)\n",
    "    targets.columns=['title']\n",
    "\n",
    "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "    out['date'] = pd.to_datetime('now')\n",
    "    out.to_csv('20_newsgroup.csv')\n",
    "    \n",
    "twenty_newsgroup_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news20dataset = pd.read_csv('20_newsgroup.csv').rename(columns={'Unnamed: 0':'id'})\n",
    "news20dataset. head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def preprocess(raw_text):\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "    \n",
    "    #remove letters\n",
    "    meaningful_words = [w for w in meaningful_words if len(w)>3]\n",
    "    \n",
    "#     #stemmed words\n",
    "#     ps = PorterStemmer()\n",
    "#     stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
    "    \n",
    "    # join the cleaned words in a list\n",
    "    cleaned_word_list = \" \".join(meaningful_words)\n",
    "\n",
    "    return cleaned_word_list\n",
    "\n",
    "news20dataset['text'] = news20dataset['text'].apply(str)\n",
    "news20dataset['text_cleaned_str'] = news20dataset['text'].apply(lambda line : preprocess(line))\n",
    "news20dataset['text_cleaned_str'] = news20dataset['text_cleaned_str'].apply(str)\n",
    "\n",
    "#news20dataset.to_csv('news20dataset_cleaned.csv')\n",
    "\n",
    "dictionary_basic_words = news20dataset[['text_cleaned_str','title']].groupby(['title'])['text_cleaned_str'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# writer = pd.ExcelWriter('dico_20wordgroups.xlsx')\n",
    "# dictionary_basic_words.to_excel(writer, sheet_name='Sheet1')\n",
    "# writer.save()\n",
    "\n",
    "dictionary_basic_words.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A : Most frequency words <font color='RED'>(4.1)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictionary_basic_words = pd.read_excel('dico_20wordgroups.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_words(raw_string_text, nb_words_to_return):\n",
    "    from collections import Counter \n",
    "    \n",
    "    # split() returns list of all the words in the string \n",
    "    split_it = raw_string_text.split() \n",
    "    \n",
    "    # Pass the split_it list to instance of Counter class. \n",
    "    Counter = Counter(split_it) \n",
    "    \n",
    "    # most_common() produces k frequently encountered \n",
    "    # input values and their respective counts. \n",
    "    most_occur = Counter.most_common(nb_words_to_return) \n",
    "    \n",
    "    return(most_occur)\n",
    "\n",
    "# def remove_common_words_from_list(list_of_unique_words, list_to_filter):\n",
    "#     return(list(filter(lambda x: x in list_of_unique_words, list_to_filter)))\n",
    "\n",
    "# def keep_uncommon_most_frequent_words_from_column_of_text_in_data_frame(data_frame, name_column):\n",
    "    \n",
    "#     # 1. Transform string rows to list rows\n",
    "#     data_frame[name_column] = data_frame[name_column].apply(lambda line : line.split())\n",
    "    \n",
    "#     # 2. Get only unique words - uncommon words between each category\n",
    "#     from collections import Counter\n",
    "#     frequency = Counter(data_frame[name_column].sum())\n",
    "#     unique_words = list({word : frequency[word] for word in frequency if frequency[word] == 1 })\n",
    "    \n",
    "#     # 3. Keep away common words\n",
    "#     data_frame[name_column] = data_frame[name_column].apply(lambda line : remove_common_words_from_list(unique_words, line))\n",
    "    \n",
    "#     # 4. Return string of unique words\n",
    "#     data_frame[name_column] = data_frame[name_column].apply(lambda line : ' '.join(line))\n",
    "    \n",
    "#     return data_frame[name_column]\n",
    "\n",
    "dictionary_basic_words['most_similar_words_not_unique'] = dictionary_basic_words['text_cleaned_str'].apply(lambda line : [word[0] for word in find_most_frequent_words(line, 150)])\n",
    "# dictionary_basic_words['unique_words_per_categories'] = keep_uncommon_most_frequent_words_from_column_of_text_in_data_frame(dictionary_basic_words, 'text_cleaned_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_basic_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B : Most frequency words BUT unique in keeping with other categories  <font color='RED'>(4.1) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(list_toflatten):\n",
    "    return([item for sublist in list_toflatten for item in sublist])\n",
    "from collections import Counter \n",
    "list_most_frequent_words_not_unique_counter = dict(Counter(flatten_list(list(dictionary_basic_words['most_similar_words_not_unique']))))\n",
    "\n",
    "unique_words = list({k:v for k,v in list_most_frequent_words_not_unique_counter.items() if v == 1})\n",
    "\n",
    "list_most_frequent_words_unique = [[word for word in nested_list if word in unique_words] for nested_list in list(dictionary_basic_words['most_similar_words_not_unique'])]\n",
    "dictionary_basic_words['most_similar_words_unique_per_catergory'] = pd.Series(list_most_frequent_words_unique)\n",
    "\n",
    "# writer = pd.ExcelWriter('dico_20wordgroups.xlsx')\n",
    "# dictionary_basic_words.to_excel(writer, sheet_name='Sheet1')\n",
    "# writer.save()\n",
    "\n",
    "dictionary_basic_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.C <font color='GREEN'>[Bonus]</font> - Create a matrix of unique words per categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_words = pd.DataFrame(list(dictionary_basic_words['most_similar_words_unique_per_catergory'])).T\n",
    "most_important_words.columns = list(dictionary_basic_words['title'].drop_duplicates())\n",
    "most_important_words.head(10)\n",
    "\n",
    "# writer = pd.ExcelWriter('dico_20wordgroups_v2.xlsx')\n",
    "# most_important_words.to_excel(writer, sheet_name='Sheet1')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 : Spelling Variants <font color='RED'>(4.2.1) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spelling_variants.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def split_dataset_into_words(dataset):\n",
    "    datawords = dataset.apply(lambda x: x.split())\n",
    "    return list(datawords)\n",
    "\n",
    "#  my_list = all_incidents \n",
    "# dictionnary\n",
    "def buffer_stemmisation_keywords(my_list):\n",
    "    my_list = [item for sublist in my_list for item in sublist]\n",
    "    aux = pd.DataFrame(my_list, columns =['word'] )\n",
    "    aux['word_stemmed'] = aux['word'].apply(lambda x : stemmer.stem(x))\n",
    "    aux = aux.groupby('word_stemmed').transform(lambda x: ', '.join(x))\n",
    "    aux['word_stemmed'] = aux['word'].apply(lambda x : stemmer.stem(x.split(',')[0]))\n",
    "    aux.index = aux['word_stemmed']\n",
    "    del aux['word_stemmed']\n",
    "    my_dict = aux.to_dict('dict')['word']\n",
    "    return my_dict\n",
    "\n",
    "dictionnary_all_words_unstemmed = buffer_stemmisation_keywords(split_dataset_into_words(dictionary_basic_words['text_cleaned_str']))\n",
    "\n",
    "# Dictionnary de-duplicated\n",
    "for key, value in dictionnary_all_words_unstemmed.items():\n",
    "    new_value = value.replace(\",\", \"\")\n",
    "    new_value = list(set(value.split()))\n",
    "    new_value = list(set(map(lambda each:each.strip(\",\"), new_value)))\n",
    "    dictionnary_all_words_unstemmed[key]=new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'sure': ['surely', 'sure'],\n",
    " 'look': ['looks', 'looked', 'look', 'looking'],\n",
    " 'happen': ['happens', 'happened', 'happenings', 'happen', 'happening'],\n",
    " 'japanes': ['japanese'],\n",
    " 'citizen': ['citizens', 'citizen'],\n",
    " 'world': ['worlds', 'world'],\n",
    " 'prepar': ['prepare', 'prepared', 'preparation', 'preparations'],\n",
    " 'round': ['rounded', 'round', 'rounding', 'rounds'],\n",
    " 'peopl': ['people', 'peoples'],\n",
    " 'stick': ['sticking', 'stick'],\n",
    " 'concentr': ['concentrated', 'concentrate', 'concentrating', 'concentration'],\n",
    " 'camp': ['camps', 'camp', 'camping'],\n",
    " 'without': ['without'],\n",
    " 'trial': ['trial'],\n",
    " 'short': ['shorted', 'shorting', 'short'],\n",
    " 'step': ['step', 'stepping'],\n",
    " 'gass': ['gassed', 'gassing'],\n",
    " 'seem': ['seems', 'seeming', 'seem', 'seemed']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 : Synonyms from WordNet  <font color='RED'>(4.2.2) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms_wordnet(word):\n",
    "    from nltk.corpus import wordnet \n",
    "    syns = wordnet.synsets(word)\n",
    "    return (list(set([syns[item].lemmas()[0].name() for item in range(len(syns))])))\n",
    "\n",
    "def retrieve_synonyms_from_wordnet_dico(nested_list_of_words_per_category):\n",
    "    buffer = flatten_list(nested_list_of_words_per_category)\n",
    "    \n",
    "    dictionnary_synonyms_referential = {}\n",
    "    for i in range(len(buffer)):\n",
    "        dictionnary_synonyms_referential[buffer[i]] = get_synonyms_wordnet(buffer[i])\n",
    "    \n",
    "    return (dictionnary_synonyms_referential)\n",
    "\n",
    "all_synonyms_wordnet = retrieve_synonyms_from_wordnet_dico(dictionary_basic_words['most_similar_words_unique_per_catergory'])\n",
    "dictionary_basic_words['wordnet'] = dictionary_basic_words['most_similar_words_unique_per_catergory'].apply(lambda line : flatten_list(list(map(get_synonyms_wordnet, line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_basic_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Enrichment from a pre-trained word embedding model <font color='RED'>(4.2.4) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"glove.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import smart_open\n",
    "from sys import platform\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "def prepend_line(infile, outfile, line):\n",
    "    with open(infile, 'r', encoding=\"utf8\") as old:\n",
    "        with open(outfile, 'w', encoding=\"utf8\") as new:\n",
    "            new.write(str(line) + \"\\n\")\n",
    "            shutil.copyfileobj(old, new)\n",
    "\n",
    "def prepend_slow(infile, outfile, line):\n",
    "    with open(infile, 'r', encoding=\"utf8\") as fin:\n",
    "        with open(outfile, 'w', encoding=\"utf8\") as fout:\n",
    "            fout.write(line + \"\\n\")\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "                \n",
    "def get_lines(glove_file_name):\n",
    "\n",
    "    with smart_open.smart_open(glove_file_name, 'r', encoding=\"utf8\") as f:\n",
    "        num_lines = sum(1 for line in f)\n",
    "    with smart_open.smart_open(glove_file_name, 'r', encoding=\"utf8\") as f:\n",
    "        num_dims = len(f.readline().split()) - 1\n",
    "    return num_lines, num_dims\n",
    "\n",
    "# Input: GloVe Model File\n",
    "# More models can be downloaded from http://nlp.stanford.edu/projects/glove/\n",
    "glove_file=\"glove.6B.300d.txt\"\n",
    "\n",
    "num_lines, dims = get_lines(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output: Gensim Model text format.\n",
    "gensim_file='glove_model2.txt'\n",
    "gensim_first_line = \"{} {}\".format(num_lines, dims)\n",
    "\n",
    "# Demo: Loads the newly created glove_model.txt into gensim API.\n",
    "model=gensim.models.KeyedVectors.load_word2vec_format(glove_file,binary=False) #GloVe Mode\n",
    "\n",
    "def get_similar_words_from_glove(word, nb_of_words_to_get = 5):\n",
    "    try:\n",
    "        buffer = [word[0] for word in model.most_similar(positive=[word], topn=nb_of_words_to_get)]\n",
    "    except:\n",
    "        buffer = []\n",
    "    return buffer\n",
    "\n",
    "dictionary_basic_words['glove'] = dictionary_basic_words['most_similar_words_unique_per_catergory'].apply(lambda line : flatten_list(list(map(get_similar_words_from_glove, line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_basic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Enrichment from a new word embedding model trained on the input corpus <font color='RED'>(4.2.5) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import pandas as pd\n",
    "news20dataset = pd.read_csv('20_newsgroup.csv').rename(columns={'Unnamed: 0':'id'})\n",
    "news20dataset['text'] = news20dataset['text'].apply(str)\n",
    "\n",
    "corpus_list = list(news20dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news20dataset = pd.read_csv('20_newsgroup.csv').rename(columns={'Unnamed: 0':'id'})\n",
    "news20dataset['text'] = news20dataset['text'].apply(str)\n",
    "\n",
    "corpus_list = list(news20dataset['text'])\n",
    "\n",
    "# build a corpus for the word2vec model\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for sentence in data:\n",
    "        word_list = sentence.split(\" \")\n",
    "        corpus.append(word_list)    \n",
    "           \n",
    "    return corpus\n",
    "\n",
    "def get_similar_words_from_homemade_word2vec(word):\n",
    "    try:\n",
    "        buffer = [word[0] for word in model.wv.most_similar(positive=word)]\n",
    "    except:\n",
    "        buffer = []\n",
    "    return buffer\n",
    "\n",
    "# build a corpus for the word2vec model\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for sentence in data:\n",
    "        word_list = sentence.split(\" \")\n",
    "        corpus.append(word_list)    \n",
    "           \n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(corpus_list)\n",
    "\n",
    "model = gensim.models.Word2Vec (corpus, size=150, window=15, min_count=2, workers=10)\n",
    "model.train(corpus,total_examples=len(corpus),epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"hundred\"\n",
    "model.wv.most_similar(positive=w1)\n",
    "\n",
    "get_similar_words_from_homemade_word2vec('home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_basic_words['homemade_enrichment'] = dictionary_basic_words['most_similar_words_unique_per_catergory'].apply(lambda line : flatten_list(list(map(get_similar_words_from_homemade_word2vec, line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = pd.ExcelWriter('dictionary_20newsGroup.xlsx')\n",
    "# dictionary_basic_words.to_excel(writer, sheet_name='Sheet1')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Function-aware component <font color='RED'>(4.2.6) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{multline}\n",
    "        FAC(\\textit{w},\\textit{c}) = \\\\ \\frac{TF(w,c) - \\frac{1}{M}\\sum_{1\\leq k\\leq M}TF(w,k) }{var(TF_{-c}(w))}\n",
    "\\end{multline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_stringList_to_List(string_list):\n",
    "    buffer = ast.literal_eval(string_list)\n",
    "    return (' '.join(buffer))\n",
    "\n",
    "def convert_stringlist_to_string(dataset, column_to_apply):\n",
    "    return (dataset.apply(lambda x: x[column_to_apply] if pd.isnull(x[column_to_apply]) else convert_stringList_to_List(x[column_to_apply]), axis=1))\n",
    "\n",
    "def remove_common_words_from_list(list_of_unique_words, list_to_filter):\n",
    "    return(list(filter(lambda x: x in list_of_unique_words, list_to_filter)))\n",
    "\n",
    "def stem_my_list(list_of_words):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    ps = PorterStemmer()\n",
    "    list_of_words_stemmed = [ps.stem(word) for word in list_of_words]\n",
    "    return list_of_words_stemmed\n",
    "\n",
    "\n",
    "def calculate_how_many_words_appear_per_category(data_frame, \n",
    "                                                 name_column_text,\n",
    "                                                 name_colum_label,\n",
    "                                                 name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category',\n",
    "                                                 name_new_colum_label_buffer_to_create = 'label_to_use_for_outer_join'):\n",
    "    \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s1 = data_frame.copy()\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    # data_frame[name_column] = data_frame[name_column].apply(lambda line : line.split())\n",
    "    data_frame_s1[name_column_text] = convert_stringlist_to_string(data_frame_s1, name_column_text).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame_s1[name_column_text] = data_frame_s1[name_column_text].apply(lambda line : stem_my_list(line))\n",
    "\n",
    "    # 3. How a given word appears per category\n",
    "    from collections import Counter\n",
    "    data_frame_s1[name_new_colum_counter_to_create] = data_frame_s1[name_column_text].apply(lambda line : Counter(line))\n",
    "    \n",
    "    # 4. Find category of works\n",
    "    data_frame_s1[name_new_colum_label_buffer_to_create] = data_frame_s1[name_colum_label].apply(lambda line: [category for category in data_frame_s1[name_colum_label].unique().tolist() if category != line])\n",
    "    data_frame_s2 = data_frame_s1.copy()\n",
    "    \n",
    "    # 5. Compute how many times a words appear in other category outer the category under consideration\n",
    "    \n",
    "    return data_frame_s1\n",
    "\n",
    "def calculate_how_many_words_appear_outer_category(data_frame_from_before, \n",
    "                                                   name_colum_text,\n",
    "                                                   name_colum_label, \n",
    "                                                   list_label_to_use_for_outer_join,\n",
    "                                                   name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category',\n",
    "                                                   name_new_colum_counter_to_create_OUTER = 'how_many_a_word_appear_per_category_OUTER'):\n",
    "    \n",
    "    # 1. Get dataframe regarding how each word occurs per category out of category\n",
    "    dataframe_filtered = data_frame_from_before[data_frame_from_before[name_colum_label].isin(list_label_to_use_for_outer_join)]\n",
    "    \n",
    "    # 2. Change format of dictionary to operate aggregation of list\n",
    "    dataframe_filtered[name_new_colum_counter_to_create_OUTER] = dataframe_filtered[name_new_colum_counter_to_create].apply(lambda line : list(line.items()))\n",
    "    \n",
    "    # 3. Calculation\n",
    "    from collections import Counter\n",
    "    inp = [ dict([i]) for i in tuple(dataframe_filtered[name_new_colum_counter_to_create_OUTER].sum()) ]\n",
    "    count = Counter()\n",
    "    for y in inp:\n",
    "        count += Counter(y)\n",
    "    \n",
    "    G = len(dataframe[name_colum_label].unique().tolist())\n",
    "    # 4. Divide by number of categories\n",
    "    sum_all_words_outer_a_given_category = {k: v/G for k, v in count.items()}\n",
    "        \n",
    "    return sum_all_words_outer_a_given_category\n",
    "\n",
    "def FAC(data_frame, \n",
    "        name_column_text,\n",
    "        name_colum_label,\n",
    "        name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category',\n",
    "        name_new_colum_label_buffer_to_create = 'label_to_use_for_outer_join'): \n",
    "    \n",
    "    dataframe_buffer = calculate_how_many_words_appear_per_category(data_frame, name_column_text, name_colum_label)\n",
    "    \n",
    "    OUTER = []\n",
    "    for item_list_category in dataframe_buffer[name_new_colum_label_buffer_to_create]:\n",
    "        OUTER.append(calculate_how_many_words_appear_outer_category(dataframe_buffer, name_column_text, name_colum_label, item_list_category))\n",
    "    \n",
    "    dataframe_buffer['RESULT_OUTER'] = pd.Series(OUTER)\n",
    "    return dataframe_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. TF-ICF (Term Frequency - Inverse Category Frequency) <font color='RED'>(4.2.6) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_stringList_to_List(string_list):\n",
    "    buffer = ast.literal_eval(string_list)\n",
    "    return (' '.join(buffer))\n",
    "\n",
    "def convert_stringlist_to_string(dataset, column_to_apply):\n",
    "    return (dataset.apply(lambda x: x[column_to_apply] if pd.isnull(x[column_to_apply]) else convert_stringList_to_List(x[column_to_apply]), axis=1))\n",
    "\n",
    "def remove_common_words_from_list(list_of_unique_words, list_to_filter):\n",
    "    return(list(filter(lambda x: x in list_of_unique_words, list_to_filter)))\n",
    "\n",
    "def stem_my_list(list_of_words):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    ps = PorterStemmer()\n",
    "    list_of_words_stemmed = [ps.stem(word) for word in list_of_words]\n",
    "    return list_of_words_stemmed\n",
    "\n",
    "def popularity_of_words_per_category(data_frame, name_column_text):\n",
    "    \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s1 = data_frame.copy()\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    # data_frame[name_column] = data_frame[name_column].apply(lambda line : line.split())\n",
    "    data_frame_s1[name_column_text] = convert_stringlist_to_string(data_frame_s1, name_column_text).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame_s1[name_column_text] = data_frame_s1[name_column_text].apply(lambda line : stem_my_list(line))\n",
    "    \n",
    "    # 3. Keep only one word of a given spelling variant\n",
    "    data_frame_s1[name_column_text] = data_frame_s1[name_column_text].apply(lambda line : sorted(set(line), key=lambda x:line.index(x)))\n",
    "    \n",
    "    # 4. How a given word appears in all category? The max is the number of categories\n",
    "    from collections import Counter\n",
    "    frequency = Counter(data_frame_s1[name_column_text].sum())\n",
    "    \n",
    "    # 5. Calculate inverse_category_frequency per words\n",
    "    popularity_per_words = {k: v for k, v in frequency.items()}\n",
    "    \n",
    "    return popularity_per_words\n",
    "\n",
    "def inverse_category_frequency(popularity_per_words, nb_labels):\n",
    "    import math\n",
    "    inverse_category_frequency_per_words = {k: math.log(nb_labels/v) for k, v in popularity_per_words.items()}\n",
    "    return inverse_category_frequency_per_words\n",
    "\n",
    "def calculate_how_many_words_appear_per_category(data_frame, name_column_text, name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category'):\n",
    "        \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s2 = data_frame.copy()\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    data_frame_s2[name_column_text] = convert_stringlist_to_string(data_frame_s2, name_column_text).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame_s2[name_column_text] = data_frame_s2[name_column_text].apply(lambda line : stem_my_list(line))\n",
    "\n",
    "    # 3. How a given word appears per category\n",
    "    from collections import Counter\n",
    "    data_frame_s2[name_new_colum_counter_to_create] = data_frame_s2[name_column_text].apply(lambda line : Counter(line))\n",
    "    \n",
    "    return data_frame_s2\n",
    "\n",
    "def TFICF(data_frame, \n",
    "          name_column_text, \n",
    "          nb_labels,\n",
    "          threshold,\n",
    "          name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category'):\n",
    "    \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s3 = data_frame.copy()\n",
    "    \n",
    "    # 1. Calculate inverse_category_frequency\n",
    "    inverse_category_frequency_per_words = popularity_of_words_per_category(data_frame_s3, name_column_text)\n",
    "    inverse_category_frequency_per_words = inverse_category_frequency(inverse_category_frequency_per_words, nb_labels)\n",
    "    \n",
    "    # 2. Term frequency per word and per category\n",
    "    data_frame_s4 = calculate_how_many_words_appear_per_category(data_frame_s3, name_column_text, name_new_colum_counter_to_create)\n",
    "    \n",
    "    # 3. Compute Term Frequency Inverse Category Frequency\n",
    "    data_frame_s4['RESULT'] = data_frame_s4[name_new_colum_counter_to_create].apply(lambda line : {k: line[k]*inverse_category_frequency_per_words[k] for k in line})\n",
    "    \n",
    "    # 4. Filter my dictionary\n",
    "    data_frame_s4['RESULT_v2'] = data_frame_s4['RESULT'].apply(lambda line : {k for k, v in line.items() if v > threshold})\n",
    "    \n",
    "    return data_frame_s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing before merging <font color='RED'>(4.3) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def preprocess_with_stemming(raw_text):\n",
    "    \n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if w not in stopword_set]\n",
    "    \n",
    "    #remove letters\n",
    "    meaningful_words = [w for w in meaningful_words if len(w)>3]\n",
    "    \n",
    "    #stemmed words\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(word) for word in meaningful_words]\n",
    "    \n",
    "    stemmed_words_unique = list(set(stemmed_words))\n",
    "    \n",
    "    # join the cleaned words in a list\n",
    "    cleaned_str = \" \".join(stemmed_words)\n",
    "\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_stringList_to_List(string_list):\n",
    "    buffer = ast.literal_eval(string_list)\n",
    "    return (' '.join(buffer))\n",
    "\n",
    "def convert_stringlist_to_string(dataset, column_to_apply):\n",
    "    return (dataset.apply(lambda x: x[column_to_apply] if pd.isnull(x[column_to_apply]) else convert_stringList_to_List(x[column_to_apply]), axis=1))\n",
    "\n",
    "def transform_column_into_string(dataset, column):\n",
    "    dataset[column] = convert_stringlist_to_string(dataset, column)\n",
    "    dataset[column] = dataset[column].apply(str)\n",
    "    dataset[column] = dataset[column].apply(lambda line : preprocess_with_stemming(line))\n",
    "    return dataset[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "dictionary_basic_words = pd.read_excel('C:/Users/c28742/Downloads/research_paper/dictionary_20newsGroup.xlsx')\n",
    "\n",
    "dictionary_basic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_to_transform = ['most_similar_words_unique_per_catergory', 'wordnet', 'glove', 'homemade_enrichment']\n",
    "column_to_transform = ['most_similar_words_unique_per_catergory', 'glove', 'homemade_enrichment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in column_to_transform:\n",
    "    transform_column_into_string(dictionary_basic_words, item)\n",
    "\n",
    "def create_final_dico(dataset, list_columns_in_final_dico):\n",
    "    dataset['final_dico'] = dataset[list_columns_in_final_dico].apply(lambda line : ' '.join(line), axis = 1)\n",
    "    dataset['final_dico'] = dataset['final_dico'].apply(lambda line : list(set(line.split())))\n",
    "    dataset['final_dico'] = dataset['final_dico'].apply(lambda line : ' '.join(line))\n",
    "    return dataset\n",
    "\n",
    "dictionary_basic_words = create_final_dico(dictionary_basic_words, column_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('final_dico.xlsx')\n",
    "dictionary_basic_words.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text similarity using LSI and cosine similarity <font color='RED'>(4.2.7) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from tqdm import tqdm_notebook \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def remove_stop_words(words_list, stopw):\n",
    "    return list(set(words_list)-set(stopw))\n",
    "def stemming(words_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in words_list]\n",
    "def tokenize_column(colonne, stem=True):\n",
    "    if (stem):\n",
    "        return colonne.str.replace(punctuation,\" \").str.replace(\"[0-9]+\",\" \").str.lower().            apply(word_tokenize).            apply(set).            apply(remove_stop_words, stopw=stopwords.words('english')).            apply(stemming)\n",
    "    else:\n",
    "        return colonne.str.replace(punctuation,\" \").str.replace(\"[0-9]+\",\" \").str.lower().            apply(word_tokenize).            apply(set).            apply(remove_stop_words, stopw=stopwords.words('english'))\n",
    "    \n",
    "def intersection_similarity(df_cand, df_src, colnames, score_name=\"score_sim_inter\"):\n",
    "    # constants\n",
    "    column_merge = \"column_merge\"\n",
    "    # make copy of dataframes\n",
    "   df_c=df_cand.copy()\n",
    "    df_s=df_src.copy()\n",
    "    # joint both dataframe (cartesian product)\n",
    "    df_c = df_c[[colnames[id_cand], colnames[tokens_cand]]]\n",
    "    df_c[column_merge]=1\n",
    "    df_s = df_s[[colnames[id_src], colnames[tokens_src]]]\n",
    "    df_s[column_merge]=1\n",
    "    output = df_c.merge(df_s, on=column_merge).drop(labels=[column_merge], axis=1)\n",
    "    # compute similarity\n",
    "    output[score_names[\"inter\"]] = output.apply(lambda row: 0 if (min(len(set(row[colnames[tokens_cand]])), len((set(row[colnames[tokens_src]]))))==0) else len(set(row[colnames[tokens_cand]]).intersection(set(row[colnames[tokens_src]])))/min(len(set(row[colnames[tokens_cand]])), len((set(row[colnames[tokens_src]])))), axis=1) \n",
    "    return output\n",
    "\n",
    "def cosine_similarity(df_c, df_s, colnames, score_name=\"score_sim_cos\"):\n",
    "    # constants\n",
    "    vec_bow_cand = \"vec_bow_cand\"\n",
    "    vec_lsi_src = \"vec_lsi_src\"\n",
    "    vec_columns = {vec_bow_cand:id_cand+\"_vec_bow\", vec_lsi_src:id_src+\"_vec_lsi\"}\n",
    "    # build dictionary\n",
    "    dictionary = corpora.Dictionary(df_c[colnames[tokens_cand]])\n",
    "    # compute vec_bow\n",
    "    df_c[vec_columns[vec_bow_cand]] = df_c[colnames[tokens_cand]].map(dictionary.doc2bow)\n",
    "    # compute lsi model\n",
    "    lsi = models.LsiModel(df_c[vec_columns[vec_bow_cand]], id2word=dictionary, num_topics=100)\n",
    "    # compute vec_lsi src\n",
    "    df_s[vec_columns[vec_lsi_src]] = list(lsi[df_s[colnames[tokens_src]].map(dictionary.doc2bow)])\n",
    "    # compute vec_lsi cand\n",
    "    index = similarities.MatrixSimilarity(lsi[df_c[vec_columns[vec_bow_cand]]])\n",
    "    # compute cosine similarity score\n",
    "    output = np.round_(index[df_s[vec_columns[vec_lsi_src]]], 3)\n",
    "    # build the output data frame\n",
    "    myShape = output.shape\n",
    "    output = pd.DataFrame(output.reshape((output.shape[0]*output.shape[1], 1), order='C'))\n",
    "    output.columns=[score_name]\n",
    "    output[colnames[id_cand]]=list(df_c[colnames[id_cand]])*myShape[0]\n",
    "    output[colnames[id_src]]=[v for v in df_s[colnames[id_src]] for _ in range(myShape[1])]\n",
    "    return output\n",
    "\n",
    "# remove identical ids\n",
    "def different_id_row(row, id1, id2):\n",
    "    return row[id1]!=row[id2]\n",
    "\n",
    "def top_k_similarity_word2vec(df_cand, df_src, colnames, score_names={\"cos\":\"score_sim_cos\",\"inter\":\"score_sim_inter\"}, top_k=10):\n",
    "    # copy of current data frame\n",
    "    df_c = df_cand.copy()\n",
    "    df_s = df_src.copy()\n",
    "    # column names of tokenized string\n",
    "    colnames2 = {id_cand: colnames[id_cand], id_src:colnames[id_src], tokens_cand:colnames[text_cand]+\"_tokens\", tokens_src:colnames[text_src]+\"_tokens\"}\n",
    "    \n",
    "    # cleaning, applying to lower, tokenizing, removing stop word, stemming\n",
    "    df_c[colnames2[tokens_cand]] = tokenize_column(df_c[colnames[text_cand]])\n",
    "    df_s[colnames2[tokens_src ]] = tokenize_column(df_s[colnames[text_src]])\n",
    "    \n",
    "    # compute cosine similarity score\n",
    "    df_cos = cosine_similarity(df_c, df_s, colnames2, score_name=score_names[\"cos\"])\n",
    "    \n",
    "    # compute intersection similarity score\n",
    "    df_inter = intersection_similarity(df_c, df_s, colnames2, score_name=score_names[\"inter\"])\n",
    "\n",
    "    # merge both results\n",
    "    output = df_cos.merge(df_inter, on=[colnames[id_cand], colnames[id_src]])\n",
    "    #output = df_inter\n",
    "    #remove pairs of same reference\n",
    "    output = output[output.apply(different_id_row, id1=colnames[id_cand], id2=colnames[id_src], axis=1)]\n",
    "\n",
    "    # select top k candidates for each item from source\n",
    "    #output = output.sort_values(by=score_names[\"inter\"], ascending=False).reset_index(drop=True)\n",
    "    output = output.sort_values(by=score_names[\"inter\"], ascending=False).groupby(by=colnames[id_src]).head(top_k).reset_index(drop=True)\n",
    "    #return output[[colnames[id_src], colnames[id_cand], score_names[\"cos\"], score_names[\"inter\"]]]\n",
    "    return output[[colnames[id_src], colnames[id_cand], score_names[\"inter\"], score_names[\"cos\"]]]\n",
    "#constants\n",
    "punctuation = \"[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\r\\n]\"\n",
    "id_cand=\"id_cand\"\n",
    "id_src=\"id_src\"\n",
    "text_cand=\"text_cand\"\n",
    "text_src=\"text_src\"\n",
    "tokens_cand=\"tokens_cand\"\n",
    "tokens_src=\"tokens_src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News_cleaned = pd.read_csv('C:/Users/c28742/Downloads/research_paper/news20dataset_cleaned.csv')\n",
    "Risk_taxonomy = pd.read_excel('C:/Users/c28742/Downloads/research_paper/dico_20NewsGroup_v01.xlsx')\n",
    "\n",
    "Risk_taxonomy['dico_final_buffer'] = Risk_taxonomy['dico_final_buffer'].apply(str)\n",
    "News_cleaned['text_cleaned_str'] = News_cleaned['text_cleaned_str'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names={id_cand:'title', id_src:\"id\", text_cand:\"dico_final_buffer\", text_src:\"text_cleaned_str\"}\n",
    "score_names={\"cos\":\"score_sim_cos\", \"inter\":\"score_sim_inter\"}\n",
    "\n",
    "sim_score = top_k_similarity_word2vec(Risk_taxonomy,News_cleaned.iloc[:1] , column_names, score_names, top_k=1)\n",
    "\n",
    "for i in tqdm_notebook(range(2,len(News_cleaned))):\n",
    "    sim_score = pd.concat([sim_score,top_k_similarity_word2vec(Risk_taxonomy,News_cleaned.iloc[i-1:i] , column_names, score_names, top_k=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('result_20NewsGroup20022019.xlsx')\n",
    "sim_score.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_of_words(data_frame, name_column, nb_occurence_per_category):\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    # data_frame[name_column] = data_frame[name_column].apply(lambda line : line.split())\n",
    "    data_frame[name_column] = convert_stringlist_to_string(data_frame, name_column).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame[name_column] = data_frame[name_column].apply(lambda line : stem_my_list(line))\n",
    "    \n",
    "    # 3. Keep only one word of a given spelling variant\n",
    "    data_frame[name_column] = data_frame[name_column].apply(lambda line : sorted(set(line), key=lambda x:line.index(x)))\n",
    "    \n",
    "    # 4. How a given word appears in all category? The max is the number of categories\n",
    "    from collections import Counter\n",
    "    frequency = Counter(data_frame[name_column].sum())\n",
    "    \n",
    "    # 5. Calculate inverse_category_frequency per words\n",
    "    inverse_category_frequency_per_words = {k: 1/v for k, v in essai.items()}\n",
    "    print(inverse_category_frequency_per_words)\n",
    "    \n",
    "    # 6. Filter words which are in XX categories in common\n",
    "    occurence_words = list({word : frequency[word] for word in frequency if frequency[word] <= nb_occurence_per_category })\n",
    "    \n",
    "    # 7. Keep away common words\n",
    "    data_frame[name_column] = data_frame[name_column].apply(lambda line : remove_common_words_from_list(occurence_words, line))\n",
    "    \n",
    "    # 8. Return string of unique words\n",
    "    data_frame[name_column] = data_frame[name_column].apply(lambda line : ' '.join(line))\n",
    "    \n",
    "    return data_frame[name_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICF = Inverse Category Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_stringList_to_List(string_list):\n",
    "    buffer = ast.literal_eval(string_list)\n",
    "    return (' '.join(buffer))\n",
    "\n",
    "def convert_stringlist_to_string(dataset, column_to_apply):\n",
    "    return (dataset.apply(lambda x: x[column_to_apply] if pd.isnull(x[column_to_apply]) else convert_stringList_to_List(x[column_to_apply]), axis=1))\n",
    "\n",
    "def remove_common_words_from_list(list_of_unique_words, list_to_filter):\n",
    "    return(list(filter(lambda x: x in list_of_unique_words, list_to_filter)))\n",
    "\n",
    "def stem_my_list(list_of_words):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    ps = PorterStemmer()\n",
    "    list_of_words_stemmed = [ps.stem(word) for word in list_of_words]\n",
    "    return list_of_words_stemmed\n",
    "\n",
    "def popularity_of_words_per_category(data_frame, name_column_text):\n",
    "    \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s1 = data_frame.copy()\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    # data_frame[name_column] = data_frame[name_column].apply(lambda line : line.split())\n",
    "    data_frame_s1[name_column_text] = convert_stringlist_to_string(data_frame_s1, name_column_text).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame_s1[name_column_text] = data_frame_s1[name_column_text].apply(lambda line : stem_my_list(line))\n",
    "    \n",
    "    # 3. Keep only one word of a given spelling variant\n",
    "    data_frame_s1[name_column_text] = data_frame_s1[name_column_text].apply(lambda line : sorted(set(line), key=lambda x:line.index(x)))\n",
    "    \n",
    "    # 4. How a given word appears in all category? The max is the number of categories\n",
    "    from collections import Counter\n",
    "    frequency = Counter(data_frame_s1[name_column_text].sum())\n",
    "    \n",
    "    # 5. Calculate inverse_category_frequency per words\n",
    "    popularity_per_words = {k: v for k, v in frequency.items()}\n",
    "    \n",
    "    return popularity_per_words\n",
    "\n",
    "def inverse_category_frequency(popularity_per_words, nb_labels):\n",
    "    import math\n",
    "    inverse_category_frequency_per_words = {k: math.log(nb_labels/v) for k, v in popularity_per_words.items()}\n",
    "    return inverse_category_frequency_per_words\n",
    "\n",
    "def calculate_how_many_words_appear_per_category(data_frame, name_column_text, name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category'):\n",
    "        \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s2 = data_frame.copy()\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    data_frame_s2[name_column_text] = convert_stringlist_to_string(data_frame_s2, name_column_text).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame_s2[name_column_text] = data_frame_s2[name_column_text].apply(lambda line : stem_my_list(line))\n",
    "\n",
    "    # 3. How a given word appears per category\n",
    "    from collections import Counter\n",
    "    data_frame_s2[name_new_colum_counter_to_create] = data_frame_s2[name_column_text].apply(lambda line : Counter(line))\n",
    "    \n",
    "    return data_frame_s2\n",
    "\n",
    "def TFICF(data_frame, \n",
    "          name_column_text, \n",
    "          nb_labels,\n",
    "          threshold,\n",
    "          name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category'):\n",
    "    \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s3 = data_frame.copy()\n",
    "    \n",
    "    # 1. Calculate inverse_category_frequency\n",
    "    inverse_category_frequency_per_words = popularity_of_words_per_category(data_frame_s3, name_column_text)\n",
    "    inverse_category_frequency_per_words = inverse_category_frequency(inverse_category_frequency_per_words, nb_labels)\n",
    "    \n",
    "    # 2. Term frequency per word and per category\n",
    "    data_frame_s4 = calculate_how_many_words_appear_per_category(data_frame_s3, name_column_text, name_new_colum_counter_to_create)\n",
    "    \n",
    "    # 3. Compute Term Frequency Inverse Category Frequency\n",
    "    data_frame_s4['RESULT'] = data_frame_s4[name_new_colum_counter_to_create].apply(lambda line : {k: line[k]*inverse_category_frequency_per_words[k] for k in line})\n",
    "    \n",
    "    # 4. Filter my dictionary\n",
    "    data_frame_s4['RESULT_v2'] = data_frame_s4['RESULT'].apply(lambda line : {k for k, v in line.items() if v > threshold})\n",
    "    \n",
    "    return data_frame_s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dico = pd.read_excel('C:/Users/adsieg/Desktop/Google_snippet/dictionary_yahoo_with_word_freq.xlsx')\n",
    "dico = dico[['class_name', 'all_keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TFICF(dico, 'all_keywords', 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('zied_dico.xlsx')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Aware Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_stringList_to_List(string_list):\n",
    "    buffer = ast.literal_eval(string_list)\n",
    "    return (' '.join(buffer))\n",
    "\n",
    "def convert_stringlist_to_string(dataset, column_to_apply):\n",
    "    return (dataset.apply(lambda x: x[column_to_apply] if pd.isnull(x[column_to_apply]) else convert_stringList_to_List(x[column_to_apply]), axis=1))\n",
    "\n",
    "def remove_common_words_from_list(list_of_unique_words, list_to_filter):\n",
    "    return(list(filter(lambda x: x in list_of_unique_words, list_to_filter)))\n",
    "\n",
    "def stem_my_list(list_of_words):\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    ps = PorterStemmer()\n",
    "    list_of_words_stemmed = [ps.stem(word) for word in list_of_words]\n",
    "    return list_of_words_stemmed\n",
    "\n",
    "\n",
    "def calculate_how_many_words_appear_per_category(data_frame, \n",
    "                                                 name_column_text,\n",
    "                                                 name_colum_label,\n",
    "                                                 name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category',\n",
    "                                                 name_new_colum_label_buffer_to_create = 'label_to_use_for_outer_join'):\n",
    "    \n",
    "    # 0. Dataframe copy save\n",
    "    data_frame_s1 = data_frame.copy()\n",
    "    \n",
    "    # 1. Transform string rows to list rows\n",
    "    # data_frame[name_column] = data_frame[name_column].apply(lambda line : line.split())\n",
    "    data_frame_s1[name_column_text] = convert_stringlist_to_string(data_frame_s1, name_column_text).apply(lambda line : line.split())\n",
    "    \n",
    "    # 2. Stemm to put away spelling variants of a given word\n",
    "    data_frame_s1[name_column_text] = data_frame_s1[name_column_text].apply(lambda line : stem_my_list(line))\n",
    "\n",
    "    # 3. How a given word appears per category\n",
    "    from collections import Counter\n",
    "    data_frame_s1[name_new_colum_counter_to_create] = data_frame_s1[name_column_text].apply(lambda line : Counter(line))\n",
    "    \n",
    "    # 4. Find category of works\n",
    "    data_frame_s1[name_new_colum_label_buffer_to_create] = data_frame_s1[name_colum_label].apply(lambda line: [category for category in data_frame_s1[name_colum_label].unique().tolist() if category != line])\n",
    "    data_frame_s2 = data_frame_s1.copy()\n",
    "    \n",
    "    # 5. Compute how many times a words appear in other category outer the category under consideration\n",
    "    \n",
    "    return data_frame_s1\n",
    "\n",
    "def calculate_how_many_words_appear_outer_category(data_frame_from_before, \n",
    "                                                   name_colum_text,\n",
    "                                                   name_colum_label, \n",
    "                                                   list_label_to_use_for_outer_join,\n",
    "                                                   name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category',\n",
    "                                                   name_new_colum_counter_to_create_OUTER = 'how_many_a_word_appear_per_category_OUTER'):\n",
    "    \n",
    "    # 1. Get dataframe regarding how each word occurs per category out of category\n",
    "    dataframe_filtered = data_frame_from_before[data_frame_from_before[name_colum_label].isin(list_label_to_use_for_outer_join)]\n",
    "    \n",
    "    # 2. Change format of dictionary to operate aggregation of list\n",
    "    dataframe_filtered[name_new_colum_counter_to_create_OUTER] = dataframe_filtered[name_new_colum_counter_to_create].apply(lambda line : list(line.items()))\n",
    "    \n",
    "    # 3. Calculation\n",
    "    from collections import Counter\n",
    "    inp = [ dict([i]) for i in tuple(dataframe_filtered[name_new_colum_counter_to_create_OUTER].sum()) ]\n",
    "    count = Counter()\n",
    "    for y in inp:\n",
    "        count += Counter(y)\n",
    "    \n",
    "    G = len(dataframe[name_colum_label].unique().tolist())\n",
    "    # 4. Divide by number of categories\n",
    "    sum_all_words_outer_a_given_category = {k: v/G for k, v in count.items()}\n",
    "        \n",
    "    return sum_all_words_outer_a_given_category\n",
    "\n",
    "def FAC(data_frame, \n",
    "        name_column_text,\n",
    "        name_colum_label,\n",
    "        name_new_colum_counter_to_create = 'how_many_a_word_appear_per_category',\n",
    "        name_new_colum_label_buffer_to_create = 'label_to_use_for_outer_join'): \n",
    "    \n",
    "    dataframe_buffer = calculate_how_many_words_appear_per_category(data_frame, name_column_text, name_colum_label)\n",
    "    \n",
    "    OUTER = []\n",
    "    for item_list_category in dataframe_buffer[name_new_colum_label_buffer_to_create]:\n",
    "        OUTER.append(calculate_how_many_words_appear_outer_category(dataframe_buffer, name_column_text, name_colum_label, item_list_category))\n",
    "    \n",
    "    dataframe_buffer['RESULT_OUTER'] = pd.Series(OUTER)\n",
    "    return dataframe_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dico = pd.read_excel('C:/Users/adsieg/Desktop/Google_snippet/dictionary_yahoo_with_word_freq.xlsx')\n",
    "dico = dico[['class_name', 'all_keywords']]\n",
    "\n",
    "list_to_consider = ['Business & Finance','Computers & Internet','Education & Reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adrien = FAC(dico, 'all_keywords', 'class_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adrien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
